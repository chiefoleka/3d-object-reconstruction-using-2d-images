{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -nq data/chair_all.zip -d data # terminal command to unzip dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "import utils\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_len = 32\n",
    "epoch_count = 400\n",
    "batch_size = 128\n",
    "noise_size = 200\n",
    "d_lr = 0.00005 # discriminator learning rate\n",
    "g_lr = 0.0025 # generator learning rate\n",
    "log_folder = \"logs/\"\n",
    "\n",
    "condition_count = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "all_models1 = utils.load_all(\"data/chair_all\", contains = '_1.mat') # names ends with a rotation number for 12 rotations, 30 degrees each\n",
    "all_models7 = utils.load_all(\"data/chair_all\", contains = '_7.mat') # 1 and 7 are 0 and 180 degrees respectively\n",
    "\n",
    "train_set1 = torch.from_numpy(all_models1).float()\n",
    "train_set7 = torch.from_numpy(all_models7).float()\n",
    "\n",
    "train_set_all = TensorDataset(train_set1, train_set7)\n",
    "train_loader = DataLoader(dataset=train_set_all, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "generator = models.Generator(noise_size=(noise_size + 1), cube_resolution=cube_len) # noise size +1 condition value\n",
    "discriminator = models.Discriminator(cube_resolution=cube_len)\n",
    "\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam(discriminator.parameters(), lr=d_lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=g_lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "criterion_GAN = torch.nn.BCELoss()\n",
    "\n",
    "def get_gan_loss(tensor,ones):\n",
    "    if(ones):\n",
    "        return criterion_GAN(tensor,Variable(torch.ones_like(tensor.data).to(device), requires_grad=False))\n",
    "    else:\n",
    "        return criterion_GAN(tensor,Variable(torch.zeros_like(tensor.data).to(device), requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(b_size = batch_size):\n",
    "    return torch.randn([b_size,noise_size], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GAN_epoch():\n",
    "    \n",
    "    g_loss = []\n",
    "    d_loss = []\n",
    "    gen_out = []\n",
    "    train_disc = True\n",
    "    \n",
    "    for i, data_c in enumerate(train_loader):\n",
    "        \n",
    "        acc_list = []\n",
    "        \n",
    "        for c in range(condition_count): # train GAN for each condition\n",
    "            \n",
    "            data = data_c[c].to(device)\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "            Dr_output = discriminator(data, c)\n",
    "            errD_real = get_gan_loss(Dr_output,True)\n",
    "            \n",
    "            fake = generator(get_noise(data.shape[0]), c)\n",
    "            Df_output = discriminator(fake.detach(), c)\n",
    "            errD_fake = get_gan_loss(Df_output,False)\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "                \n",
    "            acc_r = Dr_output.mean().item() \n",
    "            acc_f = 1.0 - Df_output.mean().item() \n",
    "            acc = (acc_r + acc_f) / 2.0\n",
    "            \n",
    "            acc_list.append(acc) # calculate discriminator accuracy\n",
    "            \n",
    "            if (train_disc): # train discriminator if the last batch accuracy is less than 0.95\n",
    "                errD.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "            generator.zero_grad() # train generator\n",
    "            fake = generator(get_noise(), c)\n",
    "            DGF_output = discriminator(fake, c)\n",
    "            errG = get_gan_loss(DGF_output,True)\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "            \n",
    "            d_loss.append(errD.mean().item())\n",
    "            g_loss.append(errG.mean().item())\n",
    "\n",
    "        generator.zero_grad() # train generator for combined loss\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        fix_noise = get_noise()\n",
    "\n",
    "        fake0 = generator(fix_noise, 0) # generate for condition 0 and 1\n",
    "        fake1 = generator(fix_noise, 1)\n",
    "        \n",
    "        fake1_rot = torch.rot90(fake1, 2, [1, 2]) # rotate condition 1\n",
    "        fake_combined = (fake0 + fake1_rot) / 2.0 # combine them by averaging\n",
    "         \n",
    "        DGF_output_c = discriminator(fake_combined, 0) # train generator for combined output\n",
    "        errG_c = get_gan_loss(DGF_output_c,True)\n",
    "        errG_c.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        train_disc = np.mean(acc_list) < 0.95 # decide for the next batch\n",
    "    \n",
    "    gen_out.append( fake0.detach().cpu() ) # return generated samples for condition 0, 1 and combined\n",
    "    gen_out.append( fake1.detach().cpu() )\n",
    "    gen_out.append( fake_combined.detach().cpu() )\n",
    "    \n",
    "    return np.mean(d_loss), np.mean(g_loss) , gen_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.clear_folder(log_folder) # create log folder\n",
    "log_file = open(log_folder +\"logs.txt\" ,\"a\") # open log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_list = []\n",
    "g_list = []\n",
    "\n",
    "pbar = tqdm( range(epoch_count+1) )\n",
    "for i in pbar :\n",
    "    \n",
    "    startTime = time.time()\n",
    "    \n",
    "    d_loss, g_loss, gen = train_GAN_epoch() #train GAN for 1 epoch\n",
    "    \n",
    "    d_list.append(d_loss) # get discriminator and generator loss\n",
    "    g_list.append(g_loss)\n",
    "    \n",
    "    utils.plot_graph([d_list,g_list], log_folder + \"loss_graph\") # plot loss graph up to that epoch\n",
    "\n",
    "    epoch_time = time.time() - startTime\n",
    "    \n",
    "    writeString = \"epoch %d --> d_loss:%0.3f g_loss:%0.3f time:%0.3f\" % (i, d_loss, g_loss, epoch_time) # generate log string\n",
    "\n",
    "    pbar.set_description(writeString)\n",
    "    log_file.write(writeString + \"\\n\") # write to log file\n",
    "    log_file.flush()\n",
    "    \n",
    "    if(i%10 == 0): # save generated samples for each 10th epoch because it takes a long time to visualize the samples\n",
    "        utils.visualize_all(gen, save=True, name = log_folder + \"samples_epoch\" + str(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
